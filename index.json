[{"categories":null,"contents":"Setup Cloudflare Tunnels to your home serve Everything you need to know to get securely connect to your locally hosted application over the internet in under 30 mins.\nAt the high level, I\u0026rsquo;m running a SimpleHTTPServer locally, that i want to access over the internet from https://home.mydomainname.com\nSecondly, I\u0026rsquo;ll lock down the access to this URL to whitelisted email addresses.\n Setup Cloudflare Tunnels to your home serve  Prerequisites Local Setup cloudflared  Download install Authenticate   Setup tunnel  create credential file create config.yml Add DNS to route traffic to tunnel run the tunnel verify   Secure you application  Create Access Group Create Access Policy Verify Security Monitoring   Additional Resources    Prerequisites Before you start, make sure you:\n Add a website to Cloudflare. Change your domain nameservers to Cloudflare  Local  in this POC i\u0026rsquo;m running a SimpleHTTPServer to surface the Director listing of a folder python2 -m SimpleHTTPServer 65000  Setup cloudflared Download wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\ninstall dpkg -i cloudflared-linux-amd64.deb\nAuthenticate cloudflared tunnel login Setup tunnel create credential file cloudflared tunnel create \u0026lt;NAME\u0026gt; this will spit out a \u0026lt;UUID\u0026gt;.json file in your ~/.clouflared DIR\ncreate config.yml tunnel: \u0026lt;Tunnel-UUID\u0026gt; credentials-file: /root/.cloudflared/\u0026lt;Tunnel-UUID\u0026gt;.json ingress: - hostname: home.mydomainname.com service: http://localhost:65000 - service: http_status:400 Add DNS to route traffic to tunnel  cloudflared tunnel route dns 856e737a-1b47-4037-9e3c-1f1c7a18eeab robinvarghese.com cloudflared tunnel run   run the tunnel cloudflared tunnel run \u0026lt;UUID or NAME\u0026gt;\nverify visiting the URL now should route traffic to your local service\nNote:\nSecure you application Cloudflare Access determines who can reach your application by applying the Access policies you configure.\nCloudflare Zero Trust integrates with your organization’s identity provider to apply Zero Trust and Secure Web Gateway policies.\nIn this example, I\u0026rsquo;m using As an alternative to configuring an identity provider, Cloudflare Zero Trust can send a one-time PIN (OTP) to approved email addresses.\nCreate Access Group A group is a set of rules that can be configured once and then quickly applied across many Access applications\nCreate Access Policy An Access policy consists of an Action as well as rules which determine the scope of the action.\nVerify visiting the URL now should present you with a login page\nSecurity Monitoring You can monitor traffic in Analytics \u0026amp; Logs from the Cloudflare dashboard.\nAdditional Resources   Official Cloudflare Documentation Follow this step-by-step guide to get your first tunnel up and running using the CLI.\n  Set Up a Cloudflare Tunnel to Expose Local Servers to the Internet\n  ","permalink":"https://codein.github.io/blog/cloudflare-tunnels/","tags":["cloudflare","tunnel","server"],"title":"Setup Cloudflare Tunnels to your home server"},{"categories":null,"contents":"i love adirondacks To assist me in planning a unforgettable getaway with our family and friends, I made a list of some of the best things to do in the Adirondacks.\nPlaces Whiteface Veterans\u0026rsquo; Memorial Highway website - wikipedia\n youtube  Driving up to the top of the Whiteface along the Veterans’ Memorial Highway—New York’s fifth-highest peak at 4,867’—isn’t your typical automotive experience.\nWhiteface Memorial Hwy, Wilmington, NY 12997 C42F+X3 Wilmington, New York\nThe Wild Center - Tupper Lake, NY website - wikipedia\nThe Wild Center, formerly known as the Natural History Museum of the Adirondacks,[1] is a natural history center in Tupper Lake, New York, near the center of New York state\u0026rsquo;s Adirondack Park.\n45 Museum Dr, Tupper Lake, NY 12986\nCloudsplitter Gondola Ride - Whiteface Mountain website - youtube\nA fifteen-minute ride gliding through the air in the awe-inspiring natural beauty of the Adirondack Mountain range, the Cloudsplitter Gondola Ride transports visitors from the base of Whiteface Mountain to the peak of Little Whiteface. At the peak itself, look around and you’ll find yourself surrounded by Lake Placid and Lake Placid Village, Lake Champlain and some of the most massive peaks in all of New York State. Not a bad way to start your day at Whiteface Mountain!\n5021 Route 86 Wilmington, NY 12997\nSanta\u0026rsquo;s Workshop, North Pole, NY website - wikipedia\n youtube  Santa\u0026rsquo;s Workshop in North Pole, a hamlet in Wilmington, New York, USA, is an amusement park that has been in operation since 1949. It was one of the first theme parks in the United States. It is open from June to December.\n324 Whiteface Memorial Hwy, Wilmington, NY 12997\nTrails High Falls Gorge website - wikipedia\n youtube  High Falls Gorge is a 22 acre, privately owned nature park. We provide safe trail access for all ages to an otherwise inaccessible area, with four splendid Adirondack waterfalls cascading over rocks into a deep crevice carved a billion years ago. In the shadow of Whiteface Mountain, you can take a nature walk to view the famous AuSable River as it cascades over ancient granite cliffs.\n4761 NYS Route 86 Wilmington, NY 12997\nWater Lake Placid Marina \u0026amp; Boat Tours website\nWilderness beauty. Adirondack history. Serenity. Cruise the pristine waters of Lake Placid in one of our enclosed pontoon boats.\nMirror Lake Public Beach 31 Parkside Dr, Lake Placid, NY 12946\nmt van hoevenberg Mt. Van Hoevenberg 31 Van Hoevenberg Way Lake Placid, NY 12946\nLover’s Lane website\nDifficulty Level: Easier Total Average Time \u0026amp; Distance: 45 Minutes | 1.4 Mile Loop\nExisting trail with a trailhead, register, trail markers, and based out of the Mountain Pass Lodge. Parking for hiking is in Lot 3, retail and services are available inside the Mountain Pass Lodge. This social distancing compliant trail offers an easy family hike that is a lovely journey in the Adirondack Forest on the Cross Country Side of our Olympic Venue. This is a loop hike.\nCliffside Coaster at mt van hoevenberg website\nCross Country Mountain Biking website\nIt doesn’t get more fun and invigorating than mountain biking at Mt. Van Hoevenberg! We have more than 30 km of trails that climb, dip and twist through pristine stands of Adirondack hardwoods and sky-scraping pines. If you’re a veteran trail rider, you’ll find plenty to challenge you: all the jumps, steeps and hairpin turns you could ever want and terrain that varies from open meadows to single-track in secluded forests.\nAdirondack Sky Center \u0026amp; Observatory website\nOne of the best things to do in the Adirondacks tonight is to head to the Adirondack Sky Center \u0026amp; Observatory. Aside from watching the stars, this facility offers an educational opportunity for you and your friends or family, to learn all about the beauty of our universe.\nmore\n","permalink":"https://codein.github.io/blog/i-love-adirondacks/","tags":["adirondacks"],"title":"i love adirondacks"},{"categories":null,"contents":"Single-threaded vs Multi-threading vs Multi-processing in Python  jupyter notebook link  We will try to run a few simulated processes to understand the performance difference between Single-threaded, Multi-threading and Multi-processing in Python.\nWe will learn about GIL - Alternative Python interpreters - by counting to 255 million and downloading few webpages\nimport concurrent.futures import requests import threading import time import math import random from multiprocessing import Pool # Reference and Credits # https://realpython.com/python-concurrency thread_local = threading.local() def get_session(): if not hasattr(thread_local, \u0026#34;session\u0026#34;): thread_local.session = requests.Session() return thread_local.session def download_site(url): \u0026#34;\u0026#34;\u0026#34;Function to simulate a high IO operation\u0026#34;\u0026#34;\u0026#34; start_time = time.time() session = get_session() log = None with session.get(url) as response: log = f\u0026#34;Read {len(response.content)} from {url}\u0026#34; duration = time.time() - start_time return { \u0026#39;work_start_time\u0026#39;: start_time, \u0026#39;work_duration\u0026#39;: duration, \u0026#39;work_output\u0026#39;: log, \u0026#39;work_type\u0026#39;: \u0026#39;IO\u0026#39;, } def countdown(n): \u0026#34;\u0026#34;\u0026#34;Function to simulate a CPU-bound operation\u0026#34;\u0026#34;\u0026#34; start_time = time.time() while n\u0026gt;0: n -= 1 duration = time.time() - start_time return { \u0026#39;work_start_time\u0026#39;: start_time, \u0026#39;work_duration\u0026#39;: duration, \u0026#39;work_output\u0026#39;: n, \u0026#39;work_type\u0026#39;: \u0026#39;CPU\u0026#39;, } def download_all_sites_threaded(sites): with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: return executor.map(download_site, sites) def process_work(args): (work_func, work_item) = args return work_func(work_item) def process_with_thread_pool_executor(work_load, max_workers=5): with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: result_list = list(executor.map(process_work, work_load)) return result_list def process_with_multiprocessing_pool(work_load, max_workers=5): with Pool(max_workers) as executor: result_list = list(executor.map(process_work, work_load)) return result_list def get_runtime(work_load, multithreading=False, multiprocessing=False, max_workers=1): start_time = time.time() if multithreading: results = process_with_thread_pool_executor(work_load, max_workers=max_workers) if multiprocessing: results = process_with_multiprocessing_pool(work_load, max_workers=max_workers) duration = time.time() - start_time return { \u0026#39;work_load_size\u0026#39;: len(work_load), \u0026#39;process_start_time\u0026#39;: start_time, \u0026#39;process_duration\u0026#39;: duration, \u0026#39;results\u0026#39;: results } def get_cpu_work_load(load_size=1): return [(countdown, 850000) for n in range(load_size)] def get_io_work_load(load_size=1): \u0026#34;\u0026#34;\u0026#34; A work is defined as tuple with the function and arg pair. This function returns a work_load of requested load_size \u0026#34;\u0026#34;\u0026#34; seed_sites = [ \u0026#34;https://www.jython.org\u0026#34;, \u0026#34;http://olympus.realpython.org/dice\u0026#34;, ] arg_sites = seed_sites * math.ceil(load_size/2) arg_sites = arg_sites[0:load_size] work_load = [(download_site, site) for site in arg_sites] return work_load def run_simulated_work_load(): runtimes = [] load_size = 300 print(\u0026#39;Load Size:{0}\u0026#39;.format(load_size)) io_work_load = get_io_work_load(load_size=load_size) cpu_work_load = get_cpu_work_load(load_size=load_size) io_and_cpu_work_load = io_work_load+cpu_work_load random.shuffle(io_and_cpu_work_load) work_load_label = \u0026#39;io_work_load single thread\u0026#39; runtime = get_runtime(io_work_load, multithreading=True, multiprocessing=False, max_workers=1) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;io_work_load 5 threads\u0026#39; runtime = get_runtime(io_work_load, multithreading=True, multiprocessing=False, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;io_work_load 5 process\u0026#39; runtime = get_runtime(io_work_load, multithreading=False, multiprocessing=True, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;cpu_work_load single thread\u0026#39; runtime = get_runtime(cpu_work_load, multithreading=True, multiprocessing=False, max_workers=1) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;cpu_work_load 5 threads\u0026#39; runtime = get_runtime(cpu_work_load, multithreading=True, multiprocessing=False, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;cpu_work_load 5 process\u0026#39; runtime = get_runtime(cpu_work_load, multithreading=False, multiprocessing=True, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;io_and_cpu_work_load single thread\u0026#39; runtime = get_runtime(io_and_cpu_work_load, multithreading=True, multiprocessing=False, max_workers=1) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;io_and_cpu_work_load 5 threads\u0026#39; runtime = get_runtime(io_and_cpu_work_load, multithreading=True, multiprocessing=False, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;io_and_cpu_work_load 5 process\u0026#39; runtime = get_runtime(io_and_cpu_work_load, multithreading=False, multiprocessing=True, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) return runtimes   ","permalink":"https://codein.github.io/blog/why-multi-process/","tags":["GIL","multiprocessing","threading","simulation"],"title":"Single-threaded vs Multi-threading vs Multi-processing in Python"},{"categories":null,"contents":"USPS API Proof of Concept  jupyter notebook link  I was hoping to demostrate how one can pull tracking information from USPS API endpoint for a given tracking ID.\nAcknowledgements  all documentation was available on this webpage https://www.usps.com/business/web-tools-apis/track-and-confirm-api.htm#_Toc41911504  ","permalink":"https://codein.github.io/blog/usps-api-proof-of-concept/","tags":["jupyter","XML","demo","USPS","API"],"title":"USPS API Proof of Concept"},{"categories":null,"contents":"Intro to SQL with 5000 movies  jupyter notebook link  This notebook is a part of a talk i have been giving to introduce people to SQL. Only pre-requisite to attend this talk is some general curiosity.\nAcknowledgements This dataset was generated from The Movie Database API. This product uses the TMDb API but is not endorsed or certified by TMDb. We used the dataset published on kaggle that includes 5000 movies\n","permalink":"https://codein.github.io/blog/intro-to-sql/","tags":["jupyter","SQL","demo","class"],"title":"Intro to SQL with 5000 movies"},{"categories":null,"contents":"Overview of SQL upsert loader   jupyter notebook link\n  source code\n  I hope to present a Proof Of Concept version of a multiprocessed data loader that I use extensively in my data integration pipelines.\nCouple of problems that it is attempting to address are\n Ability to operate on any update flat file associated with a SQL table, with minor configuration. Ability to perform upsert operations on datasets without a primary key column. Although a combination key has to be identified using multiple columns to dedup the records.  Use the link above to view the example in a jupyter notebook.\n","permalink":"https://codein.github.io/blog/sql-upsert-loader/","tags":["jupyter","SQL","ETL","integration"],"title":"SQL upsert loader"},{"categories":null,"contents":"I gave this talk to give an overview of few tools that our department relies heavily on. This talk was part of our IS department\u0026rsquo;s commons session to give employees the chance to share their skills with examples to set and follow.\nTop 5 tools Analytics uses Introduction We are hoping to give an insight into 5 tools that we rely heavily to make our life easy in Analytics department.\nI have covered 5 tools in this document\n 1 - SQL 2 - Python 3 - git 4 - jupyter 5 - Infinity Platform  Each tool has it\u0026rsquo;s on section covering\n Introduction Analytics Application Getting Started Application ideas - Where can i use it? Resources  If you have questions or need help using any of these, please feel free to reach our to Robin Varghese, it will be my at-most joy to assist you in your endeavor.\n1 SQL 1.a Introduction SQL is a database language. SQL is used to communicate with a RDBMS database(i.e. Microsoft SQL Server, MS Access, MySQL, Oracle, Sybase, Ingres). We live in a data-driven world: We at Arnot search through data to find insights to inform strategy, marketing, operations, and a plethora of other categories. We have a several systems that use large, relational databases, which makes a basic understanding of SQL a great employable skill not only for data analyst, but for almost everyone.\n1.b Analytics Application We have written close to 100k lines of SQL code in last 5 years. We modify close to 38 files with 3073 insertions(+) and 216 deletions(-) every month. SQL is a crucial tool for analyzing data.\n1.c Getting Started SQL could be distilled down to these 5 basic commands\n      SELECT column_name(s) FROM table_name WHERE condition GROUP BY column_name(s) ORDER BY column_name(s);\n      I highly recommend taking one of these online course\n Learn SQL course has been taken by 1,289,374 people needs 7 hours to complete and has no prerequisites Percipio PRACTICE LAB: Querying Data with Transact-SQL Welcome to SQL course at Khan Academy  1.d Application ideas - Where can I use it?  Make a database for a excel file that you track some data in. Connect to a database you have access to via a SQL editor(i.e. SSMS or dbeaver). Explore a database table to get some insights. Find a project to advance your SQL skills.  1.e Resources  https://en.wikipedia.org/wiki/SQL https://www.codecademy.com/learn/learn-sql https://share.percipio.com/cd/LsI05QSgc https://www.khanacademy.org/computing/computer-programming/sql/sql-basics/v/welcome-to-sql https://en.wikipedia.org/wiki/SQL_Server_Management_Studio https://dbeaver.com/ https://arnothealth.percipio.com/search?q=SQL  2 Python 2.a Introduction Python is a popular programming language. Python was designed for readability, and has some similarities to the English language.\nPython uses new lines to complete a command, as opposed to other programming languages which often use semicolons or parentheses. Python relies on indentation, using whitespace, to define scope.\nIt is used for:\n web development (server-side), software development, mathematics, system scripting.  What can Python do?\n Python can be used on a server to create web applications. Python can be used alongside software to create workflows. Python can connect to database systems. It can also read and modify files. Python can be used to handle big data and perform complex mathematics. Python can be used for rapid prototyping, or for production-ready software development.  2.b Analytics Application We use python programming language in a several ways\nData Integration Agent This generic integration agent integrates data into our data warehouse from variety of data sources( ex. sql server, csv, txt, excel files etc). It also de-dupes data it is integrating.\nFor e.g. this is how we integrate AMS credentials file to ensure we have the latest information on Arnot Health Providers.\n\u0026#39;job_key\u0026#39;:{ \u0026#39;extract_home\u0026#39;: \u0026#39;~/Projects/flat_files_folder\u0026#39;, \u0026#39;extract_filename_part\u0026#39;: \u0026#39;CREDENTIALING STATUS REPORT\u0026#39;, \u0026#39;file_extensions\u0026#39;: [\u0026#39;.xlsx\u0026#39;], \u0026#39;sheetname\u0026#39;: \u0026#39;DEMOGRAPHICS MASTER\u0026#39;, \u0026#39;skiprows\u0026#39;: 0, \u0026#39;table_name\u0026#39;: \u0026#39;[analytics_prod_database].[AMS_schema].[credentialing_master]\u0026#39;, \u0026#39;primary_keys\u0026#39;: [\u0026#39;NPI\u0026#39;], \u0026#39;severity\u0026#39;: \u0026#39;minor\u0026#39;, \u0026#39;frequency\u0026#39;: \u0026#39;weekly\u0026#39;, }, Job engine We trigger automated python jobs through our job engine. Job engine is programmed to tag each triggered job with basic diagnostic details(ex. time, host name, job_id etc). Every job also has few diagnostic tags(ex. \u0026lsquo;#critical #weekly #job #awhitt\u0026rsquo;) These tags are used by job monitor later to notify of errors or delays. Finally all job logs along with diagnostic details are pushed to a database.\nJob monitoring We monitor over 80 automated jobs across our entire infrastructure for delays and failures. Thrice a day it sends out an email with a subject similar to Job Monitor: Critical errors: 1, Minor errors: 0, Warnings: 11, Success: 85 with details on current state of jobs.\nWeb server programming Our Infinity Platform server is written in python\n2.c Getting Started I highly recommend going through one of these online material\n Percipio PRACTICE LAB: Introduction to Programming Using Python Google\u0026rsquo;s Python Class Python YouTube Tutorial This is fun and free book to get started Automate the Boring Stuff with Python - Practical Programming for Total Beginners by Al Sweigart is \u0026ldquo;written for office workers, students, administrators, and anyone who uses a computer to learn how to code small, practical programs to automate tasks on their computer.\u0026rdquo;  2.d Application ideas - Where can i use it?  Execute a Command Prompt Command from Python Controlling the Keyboard and Mouse with GUI Automation Sending Text Messages with SMS Email Gateways  2.e Resources https://en.wikipedia.org/wiki/Python_(programming_language) https://share.percipio.com/cd/cm5WXG5QF https://arnothealth.percipio.com/search?q=python https://wiki.python.org/moin/BeginnersGuide/NonProgrammers\n3 git – CVS – wiki 3.a Introduction Git is a wonderful tool for source control. Git is the most popular version control system. If you have been programming for less than a decade, it’s highly likely that you haven’t used any other method of version control. The git workflow dictates how a software team collaborates, builds, and ships software.\nWe use a Community Edition of gitlab as our Centralized Version control System(CVS) and wiki for maintaining our documents.\nWe also use gitlab to manage our teams\u0026rsquo; tasks and projects.\n3.b Analytics Application Git blame This is used to examine specific points of a file\u0026rsquo;s history and get context as to who the last author was that modified the line. This is used to explore the history of specific code and answer questions about what, how, and why the code was added to a repository.\nFor ex below file shows a history of a 122 line document how it has evolved over last 3 years.\nhttps://github.com/jupyter/notebook/blame/master/.gitignore\nGit workflow 1. Get latest master git checkout master git pull --rebase 2. Checkout remote branch Our convention is to create branches from gitlab issues\ngit checkout -t origin/\u0026lt;branch_name_here\u0026gt; 3. Commit git add 1.txt git commit -m \u0026#39;....\u0026#39; git push 3.c Getting Started  Introduction to Git Learn Git in 20 Minutes Resources to learn Git  3.d Application ideas - Where can i use it?  version controlling any document you author individually or with teammates. maintain documents to ensure up to date changes are incorporated Team management. Refer attached file Team Management Project ex. python data visualization (#1167) · Issues · AHA-dev-team _ br1ck · GitLab.pdf  3.e Resources  https://en.wikipedia.org/wiki/Git https://share.percipio.com/cd/wRBnoGBEU youtube.com/watch?v=Y9XZQO1n_7c https://try.github.io/ https://share.percipio.com/cd/wRBnoGBEU https://arnothealth.percipio.com/search?q=git  4 Jupyter 4.a Introduction Project Jupyter is three things: a collection of standards, a community, and a set of software tools. Jupyter Notebook, one part of Jupyter, is software that creates a Jupyter notebook.\nA Jupyter notebook is a document that supports mixing executable code, equations, visualizations, and narrative text. Specifically, Jupyter notebooks allow the user to bring together data, code, and prose, to tell an interactive, computational story.\n4.b Analytics Application We use this primarily when we need to attach a narrative with our analysis. Refer to attached file jupyter_notebook_demo.html\n4.c Getting Started  Jupyter Notebooks A gallery of interesting Jupyter Notebooks  4.d Application ideas - Where can i use it? This is one of the latest tools that we have uncovered so even we are learning. If you want to learn more get in touch with us we can learn this together.\n4.e Resources  https://en.wikipedia.org/wiki/Project_Jupyter https://jupyter.org/ https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks https://arnothealth.percipio.com/search?q=jupyter https://share.percipio.com/cd/vMeE9E4Dn  5 Infinity Platform 5.a Introduction Our in-house analytics platform surfaces information from most of our in house systems via 48 dashboards and calculates 355 key metrics http://infinity.aha http://8.aha\n5.b Analytics Application 5.c Getting Started   Create an Account   Give us feedback and suggestion for improvement.\n  5.d Application ideas - Where can i use it?  Learn insights about Arnot Health\u0026rsquo;s Patient Satisfaction Learn insights and metrics about our organization, specialties and providers Help us to ensure our dashboards and metrics are consistent. Identify crucial metrics in our blind-spot.  5.e Resources  http://8.aha  Thoughts  Survey MS teams or survey monkey Record Percipio Chat room IS commons  ","permalink":"https://codein.github.io/blog/top-5-tools-analytics-uses/","tags":["analytics","talk"],"title":"Analytics Commons Talk"},{"categories":null,"contents":"4 Jupyter jupyter notebook demo link\n4.a Introduction Project Jupyter is three things: a collection of standards, a community, and a set of software tools. Jupyter Notebook, one part of Jupyter, is software that creates a Jupyter notebook.\nA Jupyter notebook is a document that supports mixing executable code, equations, visualizations, and narrative text. Specifically, Jupyter notebooks allow the user to bring together data, code, and prose, to tell an interactive, computational story.\n4.b Analytics Application We use this primarily when we need to attach a narrative with our analysis. Refer to attached file jupyter_notebook_demo.html\n4.c Getting Started  Jupyter Notebooks A gallery of interesting Jupyter Notebooks  4.d Application ideas - Where can i use it? This is one of the latest tools that we have uncovered so even we are learning. If you want to learn more get in touch with us we can learn this together.\n4.e Resources  https://en.wikipedia.org/wiki/Project_Jupyter https://jupyter.org/ https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks https://arnothealth.percipio.com/search?q=jupyter https://share.percipio.com/cd/vMeE9E4Dn  ","permalink":"https://codein.github.io/blog/hello-world-jupyter/","tags":["jupyter","talk"],"title":"Jupyter - Hello world"},{"categories":null,"contents":"Maestro Job Engine Architecture +---------------------------------------------------------------------------------------------------------------------------------------------------------------+ | | | | | Maestro Job Engine Architecture | | | | | | | | +---------------------------+ | | | | | | | execute 1 | | | | | | | +---------------------------+ | | | | +---------------------------+ +------+----------+--------+--------------------------------------+-------------+ | | | | push logs to Database | date | job_name | status | tags | debug_data | | | | logging 2 | +---------------------\u0026gt; +-------------------------------------------------------------------------------+ | | | | | | | | | | | | +---------------------------+ | | | | #frequency #type #priority #author | | | | | | | | examples | | | | +---------------------------+ | | | | #daily #etl #critical #robin | | | | | | analyze logs | | | | #weekly #test #minor #bart | | | | | monitoring 3 | \u0026lt;---------------------+ | | | | #hourly #API #monitoring #dev #paul | | | | | | | | | | | | | | +----------+----------------+ +------+----------+--------+--------------------------------------+-------------+ | | | | | | | | | | | | notification | | | | | v | | +----------+----------------+ | | | | | | | Errors | | | | critical: 0 | | | | minor: 1 | | | | warning: 12 | | | | test: 1 | | | | dev: 5 | | | | success: 150 | | | | | | | +---------------------------+ | | | | | | | +---------------------------------------------------------------------------------------------------------------------------------------------------------------+ ","permalink":"https://codein.github.io/blog/maestro/","tags":["python","job engine","monitoring"],"title":"Maestro Job Engine Architecture"},{"categories":null,"contents":"Robin Varghese Whatever the problem, I want to be part of the solution. I enjoy developing software and empowering others in my knowledge space. I bring 10+ years of Full-Stack Developer experience and 5+ years of Leadership experience.\nI thrive in an environment of uncertainty which requires constant learning.\nReach Me    Github LinkedIn Email Say Hello     . . . .    Knowledge space Tech stack             Interests Languages Tools Databases Platforms   Full-stack Development Python Jupyter BigQuery Google Cloud Platform   Data Science, Analytics SQL django AWS Athena docker   Machine learning  pandas plotly mySQL Airflow   Generative AI  React.js, bootstrap MongoDB Amazon Web Services   Dev team building Javascript tornado, redis     HTML scikit-learn postgreSQL     Experience   16+ years of professional experience developing software, data analytics and solving business problems with technology.\n  9+ years of leadership experience focused on driving innovation, building and mentoring development teams.\n  Industry Dive Senior Software Engineer II Washington D.C · Mar 2021 – Present · 3+ years\n2024 (GCP, docker, python, django, react, BigQuery, jupyter, airflow)\n Discovered, scoped, found solutions, implemented and launch several nebulous projects(ex. GA4 migration, Recommendation Engine, caching layer). Architecture solutions that enables many potential futures without knowing exactly what the future is(ex Ad spot Fallback, Data Governance). Set long term technical direction to ensure our company always stays ahead of the curve.(ex. leverage our CDP to better serve our readers)  Senior Software Engineer I Washington D.C · Mar 2021 – March 2023 · 2 years\n2023 (GCP, docker, python, django, BigQuery, jupyter, airflow)\n Showcased my engineering leadership skills by guiding a team of Python developers in creating a new product using React.js(Next.js). Broke down complex problems into potential solutions, knowns, and unknowns, in order to get to solid resolutions faster(ex. partner data pipelines setup). From scratch architected and shipped large services(dev airflow and jupyter docker containers, version control repos, CI/CD pipelines, Custom airflow DAG operators, qa/prod Google composerairflow instances) required to bootstrap our data engineering infrastructure. Consolidated data from over 10+ data sources into our enterprise datalake via airflow DAGs. Developed prototypes to establish company best patterns of pipelines in and out of the datalake. Also mentored and trained team members to develop data using these patterns. Recognized as a prolific contributor to core and side projects(ex. design component editor). Consistently able to act as a force multiplier, by reducing complexity, guiding technical discussions to reach consensus and aligning the team around Objective and Key Results (ex. content gating)  2022 (GCP, python, django, BigQuery, React.js, jupyter, airflow)\n Trusted mentor and collaborator who fosters a culture of innovation and continuous learning ex. load testing, cloudflare automation, tech show and tell and reducing complexity. Identified and solved important problems on cross-cutting technical features ex. Trackers, Newsletter churn analysis, gitlab CI/CD pipeline stats etc. Inculcated a test driven development culture for internal data pipelines, products and platforms. Researched, Proposed and Delivered new technologies ex.enterprise datalake, Recommendation Engine, Airflow, Design Component System etc. Launched new publications cstoredive.com, proformative.com etc . Also created comprehensive documentation to streamline launch process. Authored several internal guidance documents to capture POCs, incidents and getting started guides.  2021 (GCP, docker, python, django, cloudflare)\n Facilitated smooth initiative deployments for high impact rollouts(ex. Rocket Loader, Proformative.com) using feature flags and RACI matrix. Developed a system to visualize and trend website performance using lighthouse, pandas and plotly. Optimized website performance based on Google Core Web Vitals and lighthouse recommendations. Built search capabilities using django haystack and websolr. Performed indexing analysis. Provided thorough and thoughtful code reviews for other engineers. Collaborated with our scrum teams to design solutions for stakeholder feature requests.  Arnot Health Manager, Business Analytics - Innovation Elmira, New York · Apr 2015 – Mar 2021 · 6 years\n2020 (AWS, SQL, python, jupyter, pandas, plotly, scikit-learn etc.)\n Served as the Principal Software Developer in Information Services(IS) department. Performed rigorous analyses on large, complex data sets for strategic initiatives(marketing, grant proposals etc.) Provided strategic insights, hypotheses, and conclusions based upon findings to leaders at all levels using ipython notebooks, sql, pandas and plotly. Developed an analytics team which excels at problem-framing, problem solving and can change direction quickly. Designed and implemented patterns of best practices for scalable, CI/CD automated, highly performant data platforms and other relevant tech stacks. Developed an enterprise wide application, to self-screen for COVID symptoms. Application platform was designed to be generously scalable and fully serverless harnessing AWS services(lambda functions, dynamodb, simple email service, etc.) Worked on several projects roadmaps and serving as a trusted committer for code for internal development.  2019 (AWS, SQL, Tableau, python, scikit-learn, jupyter, pandas, d3.js etc.)\n Lead the team developing our next generation prescriptive analytics leveraging our data lake to support chronic disease prevention and management. Automated and maintain business visualizations/dashboards/KPIs. Guide the team in enhancing our data visualization solutions using jupyter, bokeh, HoloViews and PyViz Conducted several market research analysis using Medicare claims dataset and NY SPARCS dataset Lead several projects with AWS to ensure HIPAA compliance and well architect-ed environment(ex. Athena, s3, QuickSight, IAM etc.) Navigated the department in establishing an AWS environment to offload storage and computation needs. Developed a generic event logger to capture events of interests from any place in our entire tech stack. Developed data pipelines to inject GBs of data daily from legacy systems into EDW. Developed scripts to aggregate and benchmark physicians on over 150 KPIs for Ongoing Physician Practice Evaluation.  2018 (Hadoop, SQL, Tableau, python, scikit-learn, django, coffeescript, d3.js etc.)\n Lead the team through pilot and deployment phases of an in-house Hadoop data lake. Lead the Business Analytics team through projects and providing full engagement of team management. Mentoring senior and junior developers, helped them prioritize their work, gave them actionable feedback and made sure they grow. Provide coaching and direction to analytics team in regard to best practices/approaches for software development, statistics and machine learning techniques. Surface information from across our health care system and to support data-driven decision making at all levels of Arnot Health system. Communicates findings from exploratory and predictive data analysis broadly to Arnot Health leadership. Perform market research and present quantitative analyses of health-care claim databases. Developed over 100 dashboards using tableau Build and deployed numerous production servers using pip, virtualenv and other package managers.  Sr. Integration and Database Analyst, Business Analytics 2017 (python, coffeescript, angular, d3.js, django, redis)\n Lead development of our next generation analytics platform capable of assimilating and visualize disparate data. Managed data as an enterprise asset, reducing time to find the right data/report and ensure data is trustworthy. Architected and developed a scalable analytics platform, which helps end users to locate, collaborate and share trustworthy insights in a timely fashion. Performing market research and present quantitative analyses of healthcare claims and external databases for historical analysis and trend forecasting. Researched and delivered Proof Of Concept(POC) implementations that explain key technologies. Transitioned Pilot/POC applications to DevOps team for ongoing development and lifecycle management. Developed over 20 applications to collect data via django webforms. Developed automated UI tests and UI automation jobs using Selenium. Developed ML algorithms to project KPIs(ex. revenue, volumes) Developed a unified API to pull KPIs for various enterprise entities(ex. facility, serviceline, clinic, provider etc.) using python tornado web server. Analyzed and implemented intelligent caching to reduce application load times and run time for data jobs. Developed an web application to pull data from above API to render dashboards and other data visualizations using angular.js, coffeescript, d3.js and other javascript libraries. Configured and maintain nginx servers to act as SSL endpoint, load balancer and serve web applications. Configured and apply linux command line tools to maintain prod servers using tmux, cron, systemd, bash etc. Developed and implemented authentication module to access control of API endpoints using django.  2016 (SQL, Tableau, python, scikit-learn, django)\n Developed applications to assist health care providers in advancing our Population Health Initiatives. Built and deployed machine learning models to predict patient readmissions using scikit-learn, pandas, numpy etc. Developed near real-time actionable notifications to our care-coordination team. Created several tableau dashboards to surface key information for decision makers at all levels of the organization. Work with fellow developers using agile development practices, and continually improving development methods with the goal of automating the build, integration, deployment and monitoring of jobs and Machine Learning(ML) pipelines. Designed and implemented generic parallelized data integration tools to handle ETL jobs. Deployed ML tools and encouraged their adoption across the company. Integrated and maintained over 85 data pipelines from EMR systems, external data sources, flat files etc.  2015 Business Analytics (SQL, Tableau, SSIS)\n Lead the business analytics team in developing an in-house EDW. Mentor the team to incorporate best practices for software development (GIT version control, testing, automation etc.) Architect the data pipelines and underlying process to integrated data from all major business units into EDW. Improved EDW architecture and performance. Work with data source domain experts, who understand the value potential for their data, collaborate to harvest, land and prepare that data at scale. Leveraged my technical expertise to architect and implement solutions to critical business analytics problems.(e.x. Orthopedic Serviceline dashboard)  Media Mentions Arnot Ogden Medical Center is Reducing Readmissions When a patient with four or more admissions is in the ER, a real-time alert activates and the Action Team of emergency department and outpatient case managers, community-based organizations, and physicians is mobilized\u0026hellip;see more\nArnot Health Uses Predictive Analytics to Advance Care Coordination Care coordination is essential to improving patient satisfaction and healthcare outcomes. It’s at the core of a strategic initiative that Arnot Health implemented to reduce readmissions and frequent emergency department visits at its three hospitals and 52 outpatient clinics across 55 miles\u0026hellip;see more\n THIRSTIE Software Consultant New York, New York · May 2013 – Oct 2018 · 5+ years\n Employee #5 in this 7+ year old start-up. Deployed several core services to process credit card transactions Braintree, package tracking with Glympse. Developed tools and automation scripts to onboard new licensed retail partners and sync their inventories. R\u0026amp;D for inventory management, business intelligence and software development in general.   Navisite - A Time Warner Cable Company Software Developer, Cloud R\u0026amp;D Syracuse, New York Area · Aug 2011 – Apr 2015 · 3+ yrs\n2014  NaviCloud Director (coffeescript, python, nodeJS, Selenium, RabbitMQ, Mongo DB, vmware vCloud Director etc.) Worked in our flagship(Infrastructure As A Service) product development scrum team, with a shared responsibility to deliver a next generation product. Designed and developed a caching layer to significantly reduce initial app load times, using node.js worker. Implemented a search mechanism so that users could quickly narrow down to cloud assets (vm, networking, data-center etc) R\u0026amp;D for Continuous Integration and automated tests with Selenium to suit our product. REST API development in Python on a Tornado Web server framework with a mongoDB datastore. Web app development using coffeescript, twitter bootstrap, spineJS as our MVC framework, grunt js as our build tool. Developed ansible scripts to automate frequent prod and dev tasks (ex. stock deployment, app stack updates etc.)  2013  Near realtime stream processing and analytics(clojure, python, mongoDB, HAProxy, aleph, netty, RabbitMQ) Developed a syslog event stream processing system to comply with SAS70 audit requirements. Harnessed features of load-balancers to achieve scalable and fault tolerant architecture. Developed a scalable layer to receive data stream in clojure using netty framework to push data to queue. Utilized RabbitMQ message queue to streamline events into distributed storm processing nodes. Orchestrated mongoDB clusters to map reduce and deliver near real time reports via webnoir API server. Developed automated fabric scripts to deploy, monitor and control nodes/layers in the application stack.  2012  Cloud Services Platform IaaS (Linux Apache MySQL Python, git, javascript, Java, vmware vSphere etc.) Researched and incorporated various features into our application (ex. 2 factor auth, automation scripts for customer provisioning etc.) Researched and developed several POCs, in-order to investigate new virtualization technologies.  The NaviCloud® Platform | NaviSite\nThe NaviCloud® platform sets the standard for enterprise-class infrastructure and application performance. This robust, virtualized infrastructure is deployed as multiple, secure infrastructure clouds in NaviSite\u0026rsquo;s data centers, serving as the foundation for all of NaviSite\u0026rsquo;s infrastructure, hardware, and application service offerings.\n State University of New York Research Assistant at United Health Services(UHS)\nBinghamton NY · May 2009 – Aug 2011 · 2+ year\n2011  Worked with a clinical team to understand various facets and causes of readmissions, further developed a probabilistic scoring model(LACE tool) from research to project patient\u0026rsquo;s readmission likelihood. UHS was awarded “Siemens 2011 Inspired Healthcare Outcomes Challenge” for LACE tool. Integrated more than 7 Systems into the Enterprise Data Warehouse. Assisted the financial division to analyse P\u0026amp;L, Budget formulation and project reimbursement.  2010  Internship at Katalytik Inc Android App development (Android(Client), JSON(web services) \u0026amp; Spring, hibernate, mongoDB) Co-designed and developed our core web services, desktop and mobile app for Clinical Physician Order Entry(CPOE). Designed \u0026amp; developed statistical, data mining models in SQL Server Analysis Services. Build OLAP Cubes in Business Intelligence Development Studio to support UHS in making decisions. Created and maintained Tables/Views, SQL stored procedures/queries/codes and executive dashboards. Provided DSS/Crystal Reports/OLAP Cubes training to Analysts, Super users \u0026amp; Department members. Develop and schedule SQL Server Integration Service packages to update analytical Database Servers. Demonstrated ability to investigate, analyze information and to draw conclusions.  Media Mention Siemens Names 2011 ‘Most Inspired’ Healthcare Providers United Health Services, , used Siemens Decision Support Solutions (DSS) to help reduce hospital readmissions from 9 percent in 2009 to 6 percent in 2011. DSS tools, such as stored procedures and integration services, were used to calculate a LACE (Length of stay, Acuity, Co-morbid conditions, previous Emergency department visits) score for each patient. Scores were then compiled in reports which helped to focus the attention of nursing unit care managers on the patients at highest risk. This helped remind care managers to provide education, post-discharge instructions and medication management instructions\u0026hellip;see more\nEducation Fall 2016 Harvard University Big Data in Healthcare Applications CSCI E-87 Grade A\nFall 2008 State University of New York at Binghamton 2008 - 2011 Master of Science, Computer Science GPA: 3.7\nExpert Mining - Master project  Worked under Prof. Weiyi Meng for my research on the Personal Home Page Classifier. This project was a consolidation of my curiosity in meta search engine and data mining. Prof. Lei Yu was my second advisor for my data mining efforts. Defense presentation link. R\u0026amp;D of web page classification model with Weka, openNLP, openCV and Lucene.  Summer 2003 Mumbai University - 2003 - 07 Bachelor of Engineering, Information Technology Pillai\u0026rsquo;s Institute of Information Technology - New Panvel\n","permalink":"https://codein.github.io/blog/resume/","tags":["resume"],"title":"Resume"},{"categories":null,"contents":"Addressed pretty significant page load performance issue founde in larger deployments. Eliminates uses of intensive backend query, replacing it with an asynchronous API call against a lucene index. This change reduces page load from from 2+ minutes to nearly instant, with an incredibly responsive UI.\n","permalink":"https://codein.github.io/projects/contributions/deploy-triggers/","tags":["Java","jQuery","REST APIs","Bamboo","JSON"],"title":"Atlassian Deployment Triggers"},{"categories":null,"contents":"This talk looked at Liberty Mutual’s transformation to Continuous Integration, Continuous Delivery, and DevOps. For a large, heavily regulated industry, this task can not only be daunting, but viewed by many as impossible. Often, organizations try to reduce the friction through micro-fixes, but Eddie’s team asked how to change the culture to reduce the friction and concluded with the following final points:\n Don’t mandate DevOps. Give employees the chance to master their discipline with examples to set and follow. Favor deep end-to-end accomplishments over broad but incremental steps forward. Focus on taking the right teams far before encouraging broad adoption. Centralize the platforms and tools that your teams shouldn’t be thinking about. Provide foundational services/commodities and let teams stay on purpose. Incorporate contributions from everyone; don’t stifle autonomy. Stay open to new ways of working. Challenge security policies, but respect intentions. Find new ways to enforce concerns without abandoning precaution.    ","permalink":"https://codein.github.io/publications/alldaydevops/","tags":["DevOps","Continuous Integration","Continuous Delivery","CI/CD pipelines","agile","Culture"],"title":"Organically DevOps: Building Quality and Security into the Software Supply Chain at Liberty Mutual"},{"categories":null,"contents":"Shields.io is a massive library of badges that can be inserted into project README\u0026rsquo;s or websites displaying various statuses (code coverage, health, version, etc). Support for docker was missing the current build health, and was a pretty trivial addition.\n","permalink":"https://codein.github.io/projects/contributions/shields-docker/","tags":["Docker","Rest APIs","JavaScript","node.js","JSON"],"title":"Added Docker Build Status Badge to shields.io"},{"categories":null,"contents":"While adding Structured Data to a client\u0026rsquo;s website I found some example JSON that was invalid. Simple contribution to cleanup the user documentation providing syntactically valid JSON documents.\n","permalink":"https://codein.github.io/projects/contributions/schema-org/","tags":["JSON"],"title":"Schema.org Structured Data documentation fixes"},{"categories":null,"contents":"BOSH (Bosh Outer SHell) \u0026ldquo;\u0026hellip; is an open source tool for release engineering, deployment, lifecycle management, and monitoring of distributed systems.\u0026rdquo; And it\u0026rsquo;s amazingly powerful. This examples uses BOSH to provision an Alassian vendor app running on JDK along with the support Postgres database and agents to support it. The releases manages the health of services and will automatically provision, start/stop processes across the various services.\n","permalink":"https://codein.github.io/projects/creations/bosh-agents/","tags":["DevOps","BOSH","Java","Atlassian Ecosystem","monit","python","xml/xslt","bash/shell","REST APIs"],"title":"BOSH release for Bamboo \u0026 Remote Agents"},{"categories":null,"contents":"ha (刃, edge) - the tempered cutting edge of a katana.\nScorecard metrics measurement OLAP cube sits Scorecard critical performance information on a single screen so users can monitor results in a glance.\n","permalink":"https://codein.github.io/blog/ha/","tags":null,"title":""},{"categories":null,"contents":"Multiple plugins used by thousands of teams that provide enhanced functionality of Atlassian’s core products (primarily JIRA and Bamboo) to enrich CI/CD capabilities, DevOps automation, or productivity. Functionality spans user interface, web services and persistence.\n","permalink":"https://codein.github.io/projects/creations/marketplace/","tags":["Java","Spring","REST APIs","Javascript","Atlassian Developer Ecosystem","Bamboo","JIRA","Bitbucket","Confluence","DevOps"],"title":"Atlassian Marketplace Plugins"},{"categories":null,"contents":"Provides required dependencies and additional utilities to simplify and codify the process of building, testing and delivering Atlassian plugins all the way to the live marketplace. Executes integration/AUT level tests against all stated compatible versions for the productUploads generated artifact to Atlassian marketplaceProvides corresponding metadata indicating version, release notes, and compatibility\n","permalink":"https://codein.github.io/projects/creations/docker-marketplace/","tags":["Docker","Maven","Java","Python","REST APIs","Bash/Shell"],"title":"Docker image for Bitbucket CI/CD Pipelines  \"shipit\""},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"https://codein.github.io/search/","tags":null,"title":"Search Results"}]