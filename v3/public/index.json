[{"categories":null,"contents":"Robin Varghese What ever the problem, I like to be part of the solution. I thrive in an environment of uncertainty which requires constant learning. I enjoy empowering others in my knowledge space and developing software to surface information from data. Given any problem space, my hope is to be part of the solution. My aspiration is to improve health-care delivery by harnessing my skills.\nReach Me    Github LinkedIn Email Say Hello     . . . .    Knowledge space Tech stack             Interests Languages Tools Databases Platforms   Data Science, Analytics Python Jupyter plotly MS SQL server Amazon Web Services   Project Management, SQL git, scikit-learn AWS Athena DynamoDB Tableau   Machine learning, Cloud Computing  pandas, d3 mySQL Google Data Studio   Software, Distributed System Coffeescript Angular, bootstrap MongoDB    Kaizen, dev team building Javascript tornado, django redis    Health-care, not for profit HTML RabbitMQ postgreSQL     Experience Arnot Health Manager, Business Analytics - Innovation Elmira, New York · Apr 2015 – Present · 5+ years\n2020 (AWS, SQL, python, jupyter, pandas, plotly, scikit-learn etc.)\n Serving as the Principal Software Developer in Information Services(IS) department. Performed rigorous analyses on large, complex data sets for strategic initiatives(marketing, grant proposals etc.) Provided strategic insights, hypotheses, and conclusions based upon findings to leaders at all levels using ipython notebooks, sql, pandas and plotly. Developed an analytics team which excels at problem-framing, problem solving and can change direction quickly. Designed and implemented patterns of best practices for scalable, CI/CD automated, highly performant data platforms and other relevant tech stacks. Developed an enterprise wide application, to self-screen for COVID symptoms. Application platform was designed to be generously scalable and fully serverless harnessing AWS services(lambda functions, dynamodb, simple email service, etc.) Worked on several projects roadmaps and serving as a trusted committer for code for internal development.  2019 (AWS, SQL, Tableau, python, scikit-learn, jupyter, pandas, d3.js etc.)\n Lead the team developing our next generation prescriptive analytics leveraging our data lake to support chronic disease prevention and management. Automated and maintain business visualizations/dashboards/KPIs. Guide the team in enhancing our data visualization solutions using jupyter, bokeh, HoloViews and PyViz Conducted several market research analysis using Medicare claims dataset and NY SPARCS dataset Lead several projects with AWS to ensure HIPAA compliance and well architect-ed environment(ex. Athena, s3, QuickSight, IAM etc.) Navigated the department in establishing an AWS environment to offload storage and computation needs. Developed a generic event logger to capture events of interests from any place in our entire tech stack. Developed data pipelines to inject GBs of data daily from legacy systems into EDW. Developed scripts to aggregate and benchmark physicians on over 150 KPIs for Ongoing Physician Practice Evaluation.  2018 (Hadoop, SQL, Tableau, python, scikit-learn, django, coffeescript, d3.js etc.)\n Lead the team through pilot and deployment phases of an in-house Hadoop data lake. Lead the Business Analytics team through projects and providing full engagement of team management. Mentoring senior and junior developers, helped them prioritize their work, gave them actionable feedback and made sure they grow. Provide coaching and direction to analytics team in regard to best practices/approaches for software development, statistics and machine learning techniques. Surface information from across our health care system and to support data-driven decision making at all levels of Arnot Health system. Communicates findings from exploratory and predictive data analysis broadly to Arnot Health leadership. Perform market research and present quantitative analyses of health-care claim databases. Developed over 100 dashboards using tableau Build and deployed numerous production servers using pip, virtualenv and other package managers.  Sr. Integration and Database Analyst, Business Analytics 2017 (python, coffeescript, angular, d3.js, django, redis)\n Lead development of our next generation analytics platform capable of assimilating and visualize disparate data. Managed data as an enterprise asset, reducing time to find the right data/report and ensure data is trustworthy. Architected and developed a scalable analytics platform, which helps end users to locate, collaborate and share trustworthy insights in a timely fashion. Performing market research and present quantitative analyses of healthcare claims and external databases for historical analysis and trend forecasting. Researched and delivered Proof Of Concept(POC) implementations that explain key technologies. Transitioned Pilot/POC applications to DevOps team for ongoing development and lifecycle management. Developed over 20 applications to collect data via django webforms. Developed automated UI tests and UI automation jobs using Selenium. Developed ML algorithms to project KPIs(ex. revenue, volumes) Developed a unified API to pull KPIs for various enterprise entities(ex. facility, serviceline, clinic, provider etc.) using python tornado web server. Analyzed and implemented intelligent caching to reduce application load times and run time for data jobs. Developed an web application to pull data from above API to render dashboards and other data visualizations using angular.js, coffeescript, d3.js and other javascript libraries. Configured and maintain nginx servers to act as SSL endpoint, load balancer and serve web applications. Configured and apply linux command line tools to maintain prod servers using tmux, cron, systemd, bash etc. Developed and implemented authentication module to access control of API endpoints using django.  2016 (SQL, Tableau, python, scikit-learn, django)\n Developed applications to assist health care providers in advancing our Population Health Initiatives. Built and deployed machine learning models to predict patient readmissions using scikit-learn, pandas, numpy etc. Developed near real-time actionable notifications to our care-coordination team. Created several tableau dashboards to surface key information for decision makers at all levels of the organization. Work with fellow developers using agile development practices, and continually improving development methods with the goal of automating the build, integration, deployment and monitoring of jobs and Machine Learning(ML) pipelines. Designed and implemented generic parallelized data integration tools to handle ETL jobs. Deployed ML tools and encouraged their adoption across the company. Integrated and maintained over 85 data pipelines from EMR systems, external data sources, flat files etc.  2015 Business Analytics (SQL, Tableau, SSIS)\n Lead the business analytics team in developing an in-house EDW. Mentor the team to incorporate best practices for software development (GIT version control, testing, automation etc.) Architect the data pipelines and underlying process to integrated data from all major business units into EDW. Improved EDW architecture and performance. Work with data source domain experts, who understand the value potential for their data, collaborate to harvest, land and prepare that data at scale. Leveraged my technical expertise to architect and implement solutions to critical business analytics problems.(e.x. Orthopedic Serviceline dashboard)  Media Mentions Arnot Ogden Medical Center is Reducing Readmissions When a patient with four or more admissions is in the ER, a real-time alert activates and the Action Team of emergency department and outpatient case managers, community-based organizations, and physicians is mobilized\u0026hellip;see more\nArnot Health Uses Predictive Analytics to Advance Care Coordination Care coordination is essential to improving patient satisfaction and healthcare outcomes. It’s at the core of a strategic initiative that Arnot Health implemented to reduce readmissions and frequent emergency department visits at its three hospitals and 52 outpatient clinics across 55 miles\u0026hellip;see more\n THIRSTIE Software Consultant New York, New York · May 2013 – Present · 7+ years\n One of the early employees at this 7+ year old startup. Developed and Deployed payment gateways to process credit card transactions using Braintree. Developed tools and automation scripts to onboard new licensed retail partners and sync their inventory. Developed and deployed package tracking with Glympse. R\u0026amp;D for inventory management, business intelligence and software development in general  Thirstie is technology company and e-commerce platform for the retail alcohol industry. By partnering with hundreds of licensed retail partners, we deliver products directly to consumers in 10 markets in less than an hour and ship premium alcohol products to consumers in most locations in the US and Canada in less than 3 days. Our enterprise solution, Thirstie Inside, will enable liquor brands to facilitate sales of their products through independent licensed retailers with unmatched efficiency for new consumer experience, providing visibility and transparency into data, consumer insights, analytics, and ROI. Thirstie has been recognized by WSJ, NYT, CNBC, FoxNews, and TechCrunch as the leading alcohol delivery platform in the US and Canada.\n Navisite - A Time Warner Cable Company Software Developer, Cloud R\u0026amp;D Syracuse, New York Area · Aug 2011 – Apr 2015 · 3+ yrs\n2014  NaviCloud Director (coffeescript, python, nodeJS, Selenium, RabbitMQ, Mongo DB, vmware vCloud Director etc.) Worked in our flagship(Infrastructure As A Service) product development scrum team, with a shared responsibility to deliver a next generation product. Designed and developed a caching layer to significantly reduce initial app load times, using node.js worker. Implemented a search mechanism so that users could quickly narrow down to cloud assets (vm, networking, data-center etc) R\u0026amp;D for Continuous Integration and automated tests with Selenium to suit our product. REST API development in Python on a Tornado Web server framework with a mongoDB datastore. Web app development using coffeescript, twitter bootstrap, spineJS as our MVC framework, grunt js as our build tool. Developed ansible scripts to automate frequent prod and dev tasks (ex. stock deployment, app stack updates etc.)  2013  Near realtime stream processing and analytics(clojure, python, mongoDB, HAProxy, aleph, netty, RabbitMQ) Developed a syslog event stream processing system to comply with SAS70 audit requirements. Harnessed features of load-balancers to achieve scalable and fault tolerant architecture. Developed a scalable layer to receive data stream in clojure using netty framework to push data to queue. Utilized RabbitMQ message queue to streamline events into distributed storm processing nodes. Orchestrated mongoDB clusters to map reduce and deliver near real time reports via webnoir API server. Developed automated fabric scripts to deploy, monitor and control nodes/layers in the application stack.  2012  Cloud Services Platform IaaS (Linux Apache MySQL Python, git, javascript, Java, vmware vSphere etc.) Researched and incorporated various features into our application (ex. 2 factor auth, automation scripts for customer provisioning etc.) Researched and developed several POCs, in-order to investigate new virtualization technologies.  The NaviCloud® Platform | NaviSite\nThe NaviCloud® platform sets the standard for enterprise-class infrastructure and application performance. This robust, virtualized infrastructure is deployed as multiple, secure infrastructure clouds in NaviSite\u0026rsquo;s data centers, serving as the foundation for all of NaviSite\u0026rsquo;s infrastructure, hardware, and application service offerings.\n State University of New York Research Assistant at United Health Services(UHS)\nBinghamton NY · May 2010 – Aug 2011 · 1+ year\n2011  Worked with a clinical team to understand various facets and causes of readmissions, further developed a probabilistic scoring model(LACE tool) from research to project patient\u0026rsquo;s readmission likelihood. UHS was awarded “Siemens 2011 Inspired Healthcare Outcomes Challenge” for LACE tool. Integrated more than 7 Systems into the Enterprise Data Warehouse. Assisted the financial division to analyse P\u0026amp;L, Budget formulation and project reimbursement.  2010  Android App development (Android(Client), JSON(web services) \u0026amp; Spring, hibernate, mongoDB) Co-designed and developed our core web services, desktop and mobile app for Clinical Physician Order Entry(CPOE). Designed \u0026amp; developed statistical, data mining models in SQL Server Analysis Services. Build OLAP Cubes in Business Intelligence Development Studio to support UHS in making decisions. Created and maintained Tables/Views, SQL stored procedures/queries/codes and executive dashboards. Provided DSS/Crystal Reports/OLAP Cubes training to Analysts, Super users \u0026amp; Department members. Develop and schedule SQL Server Integration Service packages to update analytical Database Servers. Demonstrated ability to investigate, analyze information and to draw conclusions.  Media Mention Siemens Names 2011 ‘Most Inspired’ Healthcare Providers United Health Services, , used Siemens Decision Support Solutions (DSS) to help reduce hospital readmissions from 9 percent in 2009 to 6 percent in 2011. DSS tools, such as stored procedures and integration services, were used to calculate a LACE (Length of stay, Acuity, Co-morbid conditions, previous Emergency department visits) score for each patient. Scores were then compiled in reports which helped to focus the attention of nursing unit care managers on the patients at highest risk. This helped remind care managers to provide education, post-discharge instructions and medication management instructions\u0026hellip;see more\nEducation Fall 2016 Harvard University Big Data in Healthcare Applications CSCI E-87 Grade A\nFall 2008 State University of New York at Binghamton 2008 - 2011 Master of Science, Computer Science GPA: 3.7\nExpert Mining - Master project  Worked under Prof. Weiyi Meng for my research on the Personal Home Page Classifier. This project was a consolidation of my curiosity in meta search engine and data mining. Prof. Lei Yu was my second advisor for my data mining efforts. Defense presentation link. R\u0026amp;D of web page classification model with Weka, openNLP, openCV and Lucene.  Summer 2003 Mumbai University - 2003 - 07 Bachelor of Engineering, Information Technology Pillai\u0026rsquo;s Institute of Information Technology - New Panvel\n","permalink":"http://codein.github.io/blog/resume/","tags":["resume"],"title":"Resume"},{"categories":null,"contents":"Single-threaded vs Multi-threading vs Multi-processing in Python  jupyter notebook link  We will try to run a few simulated processes to understand the performance difference between Single-threaded, Multi-threading and Multi-processing in Python.\nWe will learn about GIL - Alternative Python interpreters - by counting to 255 million and downloading few webpages\nimport concurrent.futures import requests import threading import time import math import random from multiprocessing import Pool # Reference and Credits # https://realpython.com/python-concurrency thread_local = threading.local() def get_session(): if not hasattr(thread_local, \u0026#34;session\u0026#34;): thread_local.session = requests.Session() return thread_local.session def download_site(url): \u0026#34;\u0026#34;\u0026#34;Function to simulate a high IO operation\u0026#34;\u0026#34;\u0026#34; start_time = time.time() session = get_session() log = None with session.get(url) as response: log = f\u0026#34;Read {len(response.content)} from {url}\u0026#34; duration = time.time() - start_time return { \u0026#39;work_start_time\u0026#39;: start_time, \u0026#39;work_duration\u0026#39;: duration, \u0026#39;work_output\u0026#39;: log, \u0026#39;work_type\u0026#39;: \u0026#39;IO\u0026#39;, } def countdown(n): \u0026#34;\u0026#34;\u0026#34;Function to simulate a CPU-bound operation\u0026#34;\u0026#34;\u0026#34; start_time = time.time() while n\u0026gt;0: n -= 1 duration = time.time() - start_time return { \u0026#39;work_start_time\u0026#39;: start_time, \u0026#39;work_duration\u0026#39;: duration, \u0026#39;work_output\u0026#39;: n, \u0026#39;work_type\u0026#39;: \u0026#39;CPU\u0026#39;, } def download_all_sites_threaded(sites): with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: return executor.map(download_site, sites) def process_work(args): (work_func, work_item) = args return work_func(work_item) def process_with_thread_pool_executor(work_load, max_workers=5): with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: result_list = list(executor.map(process_work, work_load)) return result_list def process_with_multiprocessing_pool(work_load, max_workers=5): with Pool(max_workers) as executor: result_list = list(executor.map(process_work, work_load)) return result_list def get_runtime(work_load, multithreading=False, multiprocessing=False, max_workers=1): start_time = time.time() if multithreading: results = process_with_thread_pool_executor(work_load, max_workers=max_workers) if multiprocessing: results = process_with_multiprocessing_pool(work_load, max_workers=max_workers) duration = time.time() - start_time return { \u0026#39;work_load_size\u0026#39;: len(work_load), \u0026#39;process_start_time\u0026#39;: start_time, \u0026#39;process_duration\u0026#39;: duration, \u0026#39;results\u0026#39;: results } def get_cpu_work_load(load_size=1): return [(countdown, 850000) for n in range(load_size)] def get_io_work_load(load_size=1): \u0026#34;\u0026#34;\u0026#34; A work is defined as tuple with the function and arg pair. This function returns a work_load of requested load_size \u0026#34;\u0026#34;\u0026#34; seed_sites = [ \u0026#34;https://www.jython.org\u0026#34;, \u0026#34;http://olympus.realpython.org/dice\u0026#34;, ] arg_sites = seed_sites * math.ceil(load_size/2) arg_sites = arg_sites[0:load_size] work_load = [(download_site, site) for site in arg_sites] return work_load def run_simulated_work_load(): runtimes = [] load_size = 300 print(\u0026#39;Load Size:{0}\u0026#39;.format(load_size)) io_work_load = get_io_work_load(load_size=load_size) cpu_work_load = get_cpu_work_load(load_size=load_size) io_and_cpu_work_load = io_work_load+cpu_work_load random.shuffle(io_and_cpu_work_load) work_load_label = \u0026#39;io_work_load single thread\u0026#39; runtime = get_runtime(io_work_load, multithreading=True, multiprocessing=False, max_workers=1) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;io_work_load 5 threads\u0026#39; runtime = get_runtime(io_work_load, multithreading=True, multiprocessing=False, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;io_work_load 5 process\u0026#39; runtime = get_runtime(io_work_load, multithreading=False, multiprocessing=True, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;cpu_work_load single thread\u0026#39; runtime = get_runtime(cpu_work_load, multithreading=True, multiprocessing=False, max_workers=1) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;cpu_work_load 5 threads\u0026#39; runtime = get_runtime(cpu_work_load, multithreading=True, multiprocessing=False, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;cpu_work_load 5 process\u0026#39; runtime = get_runtime(cpu_work_load, multithreading=False, multiprocessing=True, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;io_and_cpu_work_load single thread\u0026#39; runtime = get_runtime(io_and_cpu_work_load, multithreading=True, multiprocessing=False, max_workers=1) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;io_and_cpu_work_load 5 threads\u0026#39; runtime = get_runtime(io_and_cpu_work_load, multithreading=True, multiprocessing=False, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) work_load_label = \u0026#39;io_and_cpu_work_load 5 process\u0026#39; runtime = get_runtime(io_and_cpu_work_load, multithreading=False, multiprocessing=True, max_workers=5) runtime[\u0026#39;work_load_label\u0026#39;] = work_load_label print(work_load_label) runtimes.append(runtime) return runtimes ","permalink":"http://codein.github.io/blog/why-multi-process/","tags":["GIL","multiprocessing","threading","simulation"],"title":"Single-threaded vs Multi-threading vs Multi-processing in Python"},{"categories":null,"contents":"USPS API Proof of Concept  jupyter notebook link  I was hoping to demostrate how one can pull tracking information from USPS API endpoint for a given tracking ID.\nAcknowledgements  all documentation was available on this webpage https://www.usps.com/business/web-tools-apis/track-and-confirm-api.htm#_Toc41911504  ","permalink":"http://codein.github.io/blog/usps-api-proof-of-concept/","tags":["jupyter","XML","demo","USPS","API"],"title":"USPS API Proof of Concept"},{"categories":null,"contents":"Intro to SQL with 5000 movies  jupyter notebook link  This notebook is a part of a talk i have been giving to introduce people to SQL. Only pre-requisite to attend this talk is some general curiosity.\nAcknowledgements This dataset was generated from The Movie Database API. This product uses the TMDb API but is not endorsed or certified by TMDb. We used the dataset published on kaggle that includes 5000 movies\n","permalink":"http://codein.github.io/blog/intro-to-sql/","tags":["jupyter","SQL","demo","class"],"title":"Intro to SQL with 5000 movies"},{"categories":null,"contents":"Overview of SQL upsert loader   jupyter notebook link\n  source code\n  I hope to present a Proof Of Concept version of a multiprocessed data loader that I use extensively in my data integration pipelines.\nCouple of problems that it is attempting to address are\n Ability to operate on any update flat file associated with a SQL table, with minor configuration. Ability to perform upsert operations on datasets without a primary key column. Although a combination key has to be identified using multiple columns to dedup the records.  Use the link above to view the example in a jupyter notebook.\n","permalink":"http://codein.github.io/blog/sql-upsert-loader/","tags":["jupyter","SQL","ETL","integration"],"title":"SQL upsert loader"},{"categories":null,"contents":"I gave this talk to give an overview of few tools that our department relies heavily on. This talk was part of our IS department\u0026rsquo;s commons session to give employees the chance to share their skills with examples to set and follow.\nTop 5 tools Analytics uses Introduction We are hoping to give an insight into 5 tools that we rely heavily to make our life easy in Analytics department.\nI have covered 5 tools in this document\n 1 - SQL 2 - Python 3 - git 4 - jupyter 5 - Infinity Platform  Each tool has it\u0026rsquo;s on section covering\n Introduction Analytics Application Getting Started Application ideas - Where can i use it? Resources  If you have questions or need help using any of these, please feel free to reach our to Robin Varghese, it will be my at-most joy to assist you in your endeavor.\n1 SQL 1.a Introduction SQL is a database language. SQL is used to communicate with a RDBMS database(i.e. Microsoft SQL Server, MS Access, MySQL, Oracle, Sybase, Ingres). We live in a data-driven world: We at Arnot search through data to find insights to inform strategy, marketing, operations, and a plethora of other categories. We have a several systems that use large, relational databases, which makes a basic understanding of SQL a great employable skill not only for data analyst, but for almost everyone.\n1.b Analytics Application We have written close to 100k lines of SQL code in last 5 years. We modify close to 38 files with 3073 insertions(+) and 216 deletions(-) every month. SQL is a crucial tool for analyzing data.\n1.c Getting Started SQL could be distilled down to these 5 basic commands\n      SELECT column_name(s) FROM table_name WHERE condition GROUP BY column_name(s) ORDER BY column_name(s);\n      I highly recommend taking one of these online course\n Learn SQL course has been taken by 1,289,374 people needs 7 hours to complete and has no prerequisites Percipio PRACTICE LAB: Querying Data with Transact-SQL Welcome to SQL course at Khan Academy  1.d Application ideas - Where can I use it?  Make a database for a excel file that you track some data in. Connect to a database you have access to via a SQL editor(i.e. SSMS or dbeaver). Explore a database table to get some insights. Find a project to advance your SQL skills.  1.e Resources  https://en.wikipedia.org/wiki/SQL https://www.codecademy.com/learn/learn-sql https://share.percipio.com/cd/LsI05QSgc https://www.khanacademy.org/computing/computer-programming/sql/sql-basics/v/welcome-to-sql https://en.wikipedia.org/wiki/SQL_Server_Management_Studio https://dbeaver.com/ https://arnothealth.percipio.com/search?q=SQL  2 Python 2.a Introduction Python is a popular programming language. Python was designed for readability, and has some similarities to the English language.\nPython uses new lines to complete a command, as opposed to other programming languages which often use semicolons or parentheses. Python relies on indentation, using whitespace, to define scope.\nIt is used for:\n web development (server-side), software development, mathematics, system scripting.  What can Python do?\n Python can be used on a server to create web applications. Python can be used alongside software to create workflows. Python can connect to database systems. It can also read and modify files. Python can be used to handle big data and perform complex mathematics. Python can be used for rapid prototyping, or for production-ready software development.  2.b Analytics Application We use python programming language in a several ways\nData Integration Agent This generic integration agent integrates data into our data warehouse from variety of data sources( ex. sql server, csv, txt, excel files etc). It also de-dupes data it is integrating.\nFor e.g. this is how we integrate AMS credentials file to ensure we have the latest information on Arnot Health Providers.\n\u0026#39;job_key\u0026#39;:{ \u0026#39;extract_home\u0026#39;: \u0026#39;~/Projects/flat_files_folder\u0026#39;, \u0026#39;extract_filename_part\u0026#39;: \u0026#39;CREDENTIALING STATUS REPORT\u0026#39;, \u0026#39;file_extensions\u0026#39;: [\u0026#39;.xlsx\u0026#39;], \u0026#39;sheetname\u0026#39;: \u0026#39;DEMOGRAPHICS MASTER\u0026#39;, \u0026#39;skiprows\u0026#39;: 0, \u0026#39;table_name\u0026#39;: \u0026#39;[analytics_prod_database].[AMS_schema].[credentialing_master]\u0026#39;, \u0026#39;primary_keys\u0026#39;: [\u0026#39;NPI\u0026#39;], \u0026#39;severity\u0026#39;: \u0026#39;minor\u0026#39;, \u0026#39;frequency\u0026#39;: \u0026#39;weekly\u0026#39;, }, Job engine We trigger automated python jobs through our job engine. Job engine is programmed to tag each triggered job with basic diagnostic details(ex. time, host name, job_id etc). Every job also has few diagnostic tags(ex. \u0026lsquo;#critical #weekly #job #awhitt\u0026rsquo;) These tags are used by job monitor later to notify of errors or delays. Finally all job logs along with diagnostic details are pushed to a database.\nJob monitoring We monitor over 80 automated jobs across our entire infrastructure for delays and failures. Thrice a day it sends out an email with a subject similar to Job Monitor: Critical errors: 1, Minor errors: 0, Warnings: 11, Success: 85 with details on current state of jobs.\nWeb server programming Our Infinity Platform server is written in python\n2.c Getting Started I highly recommend going through one of these online material\n Percipio PRACTICE LAB: Introduction to Programming Using Python Google\u0026rsquo;s Python Class Python YouTube Tutorial This is fun and free book to get started Automate the Boring Stuff with Python - Practical Programming for Total Beginners by Al Sweigart is \u0026ldquo;written for office workers, students, administrators, and anyone who uses a computer to learn how to code small, practical programs to automate tasks on their computer.\u0026rdquo;  2.d Application ideas - Where can i use it?  Execute a Command Prompt Command from Python Controlling the Keyboard and Mouse with GUI Automation Sending Text Messages with SMS Email Gateways  2.e Resources https://en.wikipedia.org/wiki/Python_(programming_language) https://share.percipio.com/cd/cm5WXG5QF https://arnothealth.percipio.com/search?q=python https://wiki.python.org/moin/BeginnersGuide/NonProgrammers\n3 git – CVS – wiki 3.a Introduction Git is a wonderful tool for source control. Git is the most popular version control system. If you have been programming for less than a decade, it’s highly likely that you haven’t used any other method of version control. The git workflow dictates how a software team collaborates, builds, and ships software.\nWe use a Community Edition of gitlab as our Centralized Version control System(CVS) and wiki for maintaining our documents.\nWe also use gitlab to manage our teams\u0026rsquo; tasks and projects.\n3.b Analytics Application Git blame This is used to examine specific points of a file\u0026rsquo;s history and get context as to who the last author was that modified the line. This is used to explore the history of specific code and answer questions about what, how, and why the code was added to a repository.\nFor ex below file shows a history of a 122 line document how it has evolved over last 3 years.\nhttps://github.com/jupyter/notebook/blame/master/.gitignore\nGit workflow 1. Get latest master git checkout master git pull --rebase 2. Checkout remote branch Our convention is to create branches from gitlab issues\ngit checkout -t origin/\u0026lt;branch_name_here\u0026gt; 3. Commit git add 1.txt git commit -m \u0026#39;....\u0026#39; git push 3.c Getting Started  Introduction to Git Learn Git in 20 Minutes Resources to learn Git  3.d Application ideas - Where can i use it?  version controlling any document you author individually or with teammates. maintain documents to ensure up to date changes are incorporated Team management. Refer attached file Team Management Project ex. python data visualization (#1167) · Issues · AHA-dev-team _ br1ck · GitLab.pdf  3.e Resources  https://en.wikipedia.org/wiki/Git https://share.percipio.com/cd/wRBnoGBEU youtube.com/watch?v=Y9XZQO1n_7c https://try.github.io/ https://share.percipio.com/cd/wRBnoGBEU https://arnothealth.percipio.com/search?q=git  4 Jupyter 4.a Introduction Project Jupyter is three things: a collection of standards, a community, and a set of software tools. Jupyter Notebook, one part of Jupyter, is software that creates a Jupyter notebook.\nA Jupyter notebook is a document that supports mixing executable code, equations, visualizations, and narrative text. Specifically, Jupyter notebooks allow the user to bring together data, code, and prose, to tell an interactive, computational story.\n4.b Analytics Application We use this primarily when we need to attach a narrative with our analysis. Refer to attached file jupyter_notebook_demo.html\n4.c Getting Started  Jupyter Notebooks A gallery of interesting Jupyter Notebooks  4.d Application ideas - Where can i use it? This is one of the latest tools that we have uncovered so even we are learning. If you want to learn more get in touch with us we can learn this together.\n4.e Resources  https://en.wikipedia.org/wiki/Project_Jupyter https://jupyter.org/ https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks https://arnothealth.percipio.com/search?q=jupyter https://share.percipio.com/cd/vMeE9E4Dn  5 Infinity Platform 5.a Introduction Our in-house analytics platform surfaces information from most of our in house systems via 48 dashboards and calculates 355 key metrics http://infinity.aha http://8.aha\n5.b Analytics Application 5.c Getting Started   Create an Account   Give us feedback and suggestion for improvement.\n  5.d Application ideas - Where can i use it?  Learn insights about Arnot Health\u0026rsquo;s Patient Satisfaction Learn insights and metrics about our organization, specialties and providers Help us to ensure our dashboards and metrics are consistent. Identify crucial metrics in our blind-spot.  5.e Resources  http://8.aha  Thoughts  Survey MS teams or survey monkey Record Percipio Chat room IS commons  ","permalink":"http://codein.github.io/blog/top-5-tools-analytics-uses/","tags":["analytics","talk"],"title":"Analytics Commons Talk"},{"categories":null,"contents":"4 Jupyter jupyter notebook demo link\n4.a Introduction Project Jupyter is three things: a collection of standards, a community, and a set of software tools. Jupyter Notebook, one part of Jupyter, is software that creates a Jupyter notebook.\nA Jupyter notebook is a document that supports mixing executable code, equations, visualizations, and narrative text. Specifically, Jupyter notebooks allow the user to bring together data, code, and prose, to tell an interactive, computational story.\n4.b Analytics Application We use this primarily when we need to attach a narrative with our analysis. Refer to attached file jupyter_notebook_demo.html\n4.c Getting Started  Jupyter Notebooks A gallery of interesting Jupyter Notebooks  4.d Application ideas - Where can i use it? This is one of the latest tools that we have uncovered so even we are learning. If you want to learn more get in touch with us we can learn this together.\n4.e Resources  https://en.wikipedia.org/wiki/Project_Jupyter https://jupyter.org/ https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks https://arnothealth.percipio.com/search?q=jupyter https://share.percipio.com/cd/vMeE9E4Dn  ","permalink":"http://codein.github.io/blog/hello-world-jupyter/","tags":["jupyter","talk"],"title":"Jupyter - Hello world"},{"categories":null,"contents":"Maestro Job Engine Architecture +---------------------------------------------------------------------------------------------------------------------------------------------------------------+ | | | | | Maestro Job Engine Architecture | | | | | | | | +---------------------------+ | | | | | | | execute 1 | | | | | | | +---------------------------+ | | | | +---------------------------+ +------+----------+--------+--------------------------------------+-------------+ | | | | push logs to Database | date | job_name | status | tags | debug_data | | | | logging 2 | +---------------------\u0026gt; +-------------------------------------------------------------------------------+ | | | | | | | | | | | | +---------------------------+ | | | | #frequency #type #priority #author | | | | | | | | examples | | | | +---------------------------+ | | | | #daily #etl #critical #robin | | | | | | analyze logs | | | | #weekly #test #minor #bart | | | | | monitoring 3 | \u0026lt;---------------------+ | | | | #hourly #API #monitoring #dev #paul | | | | | | | | | | | | | | +----------+----------------+ +------+----------+--------+--------------------------------------+-------------+ | | | | | | | | | | | | notification | | | | | v | | +----------+----------------+ | | | | | | | Errors | | | | critical: 0 | | | | minor: 1 | | | | warning: 12 | | | | test: 1 | | | | dev: 5 | | | | success: 150 | | | | | | | +---------------------------+ | | | | | | | +---------------------------------------------------------------------------------------------------------------------------------------------------------------+ ","permalink":"http://codein.github.io/blog/maestro/","tags":["python","job engine","monitoring"],"title":"Maestro Job Engine Architecture"},{"categories":null,"contents":"Addressed pretty significant page load performance issue founde in larger deployments. Eliminates uses of intensive backend query, replacing it with an asynchronous API call against a lucene index. This change reduces page load from from 2+ minutes to nearly instant, with an incredibly responsive UI.\n","permalink":"http://codein.github.io/projects/contributions/deploy-triggers/","tags":["Java","jQuery","REST APIs","Bamboo","JSON"],"title":"Atlassian Deployment Triggers"},{"categories":null,"contents":"This talk looked at Liberty Mutual’s transformation to Continuous Integration, Continuous Delivery, and DevOps. For a large, heavily regulated industry, this task can not only be daunting, but viewed by many as impossible. Often, organizations try to reduce the friction through micro-fixes, but Eddie’s team asked how to change the culture to reduce the friction and concluded with the following final points:\n Don’t mandate DevOps. Give employees the chance to master their discipline with examples to set and follow. Favor deep end-to-end accomplishments over broad but incremental steps forward. Focus on taking the right teams far before encouraging broad adoption. Centralize the platforms and tools that your teams shouldn’t be thinking about. Provide foundational services/commodities and let teams stay on purpose. Incorporate contributions from everyone; don’t stifle autonomy. Stay open to new ways of working. Challenge security policies, but respect intentions. Find new ways to enforce concerns without abandoning precaution.    ","permalink":"http://codein.github.io/publications/alldaydevops/","tags":["DevOps","Continuous Integration","Continuous Delivery","CI/CD pipelines","agile","Culture"],"title":"Organically DevOps: Building Quality and Security into the Software Supply Chain at Liberty Mutual"},{"categories":null,"contents":"Shields.io is a massive library of badges that can be inserted into project README\u0026rsquo;s or websites displaying various statuses (code coverage, health, version, etc). Support for docker was missing the current build health, and was a pretty trivial addition.\n","permalink":"http://codein.github.io/projects/contributions/shields-docker/","tags":["Docker","Rest APIs","JavaScript","node.js","JSON"],"title":"Added Docker Build Status Badge to shields.io"},{"categories":null,"contents":"While adding Structured Data to a client\u0026rsquo;s website I found some example JSON that was invalid. Simple contribution to cleanup the user documentation providing syntactically valid JSON documents.\n","permalink":"http://codein.github.io/projects/contributions/schema-org/","tags":["JSON"],"title":"Schema.org Structured Data documentation fixes"},{"categories":null,"contents":"BOSH (Bosh Outer SHell) \u0026ldquo;\u0026hellip;is an open source tool for release engineering, deployment, lifecycle management, and monitoring of distributed systems.\u0026rdquo; And it\u0026rsquo;s amazingly powerful. This examples uses BOSH to provision an Alassian vendor app running on JDK along with the support Postgres database and agents to support it. The releases manages the health of services and will automatically provision, start/stop processes across the various services.\n","permalink":"http://codein.github.io/projects/creations/bosh-agents/","tags":["DevOps","BOSH","Java","Atlassian Ecosystem","monit","python","xml/xslt","bash/shell","REST APIs"],"title":"BOSH release for Bamboo \u0026 Remote Agents"},{"categories":null,"contents":"ha (刃, edge) - the tempered cutting edge of a katana.\nScorecard metrics measurement OLAP cube sits Scorecard critical performance information on a single screen so users can monitor results in a glance.\n","permalink":"http://codein.github.io/blog/ha/","tags":null,"title":""},{"categories":null,"contents":"Multiple plugins used by thousands of teams that provide enhanced functionality of Atlassian’s core products (primarily JIRA and Bamboo) to enrich CI/CD capabilities, DevOps automation, or productivity. Functionality spans user interface, web services and persistence.\n","permalink":"http://codein.github.io/projects/creations/marketplace/","tags":["Java","Spring","REST APIs","Javascript","Atlassian Developer Ecosystem","Bamboo","JIRA","Bitbucket","Confluence","DevOps"],"title":"Atlassian Marketplace Plugins"},{"categories":null,"contents":"Provides required dependencies and additional utilities to simplify and codify the process of building, testing and delivering Atlassian plugins all the way to the live marketplace.Executes integration/AUT level tests against all stated compatible versions for the productUploads generated artifact to Atlassian marketplaceProvides corresponding metadata indicating version, release notes, and compatibility","permalink":"http://codein.github.io/projects/creations/docker-marketplace/","tags":["Docker","Maven","Java","Python","REST APIs","Bash/Shell"],"title":"Docker image for Bitbucket CI/CD Pipelines  \"shipit\""},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"http://codein.github.io/search/","tags":null,"title":"Search Results"}]